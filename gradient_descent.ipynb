{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №1\n",
    "## Градиентный спуск\n",
    "Трутнев Алексей Игоревич вариант №5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from typing import Callable\n",
    "\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача оптимизации\n",
    "\n",
    "$\\min\\{ (x-\\mu_0)^\\top A (x-\\mu_0) : ||x||_2^2 \\leq 1 \\}$, где\n",
    "* $x \\in \\mathbb{R}^n$, $A$ - симметричная, положительно определенная матрица\n",
    "* $\\mu_0 = (1,1,\\dots,1)^\\top \\in \\mathbb{R}^n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(x: NDArray, A: NDArray, mu: NDArray) -> float:\n",
    "    return (x - mu).T @ A @ (x - mu)\n",
    "\n",
    "\n",
    "def limitation(x: NDArray) -> float:\n",
    "    return np.sum(np.power(x, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraint gradient\n",
    "$\\sum_{i=1}^{n} x_i^2 \\leq 1$, где $x = (x_1, x_2, \\dots, x_n)^\\top \\in \\mathbb{R}^n$\n",
    "\n",
    "$\\nabla = (2x_1, 2x_2, \\dots, 2x_n)^\\top \\in \\mathbb{R}^n$\n",
    "\n",
    "\n",
    "### Objective gradient\n",
    "\n",
    "$\\nabla x^\\top A x = (A+A^\\top)x$. При симметричной матрице $A$ градиент имеет вид $\\nabla x^\\top A x = 2Ax$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_function(x: NDArray, A: NDArray, mu: NDArray) -> NDArray:\n",
    "    return 2 * A @ (x - mu)\n",
    "\n",
    "\n",
    "def grad_limitation(x: NDArray) -> NDArray:\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_optimality(x: NDArray, A: NDArray, mu: NDArray) -> bool:\n",
    "    return grad_limitation(x) @ grad_function(x, A, mu) >= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Исследование на выпуклость"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Для каждого значения $n \\in \\{10, 20,\\dots, 100\\}$ сгенерируйте $N = 100$ тестовых примеров. В каждом случае найдите глобальный минимум, $x^* \\in \\mathbb{R}^n$, с помощью CVX. Проверьте, что в точке минимума выполняется условие оптимальности (т.е. вектора градиента к ограничению и антиградиента к целевой функции сонаправлены).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_symmetric(A: NDArray) -> bool:\n",
    "    return (A==A.T).all()\n",
    "\n",
    "\n",
    "def check_positive_definition(A: NDArray) -> bool:\n",
    "    n = A.shape[0]\n",
    "    test_x = np.random.random(n)\n",
    "    return (test_x.T @ A @ test_x) > 0\n",
    "\n",
    "\n",
    "def get_symmetric_matrix(n: int) -> NDArray:\n",
    "    A = np.random.rand(n, n)\n",
    "    matrix = A @ A.T\n",
    "    assert check_symmetric(matrix) and check_positive_definition(matrix)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimality condition not satisfied for n=10.\n",
      "Optimality condition not satisfied for n=20.\n",
      "Optimality condition not satisfied for n=30.\n",
      "Optimality condition not satisfied for n=40.\n",
      "Optimality condition not satisfied for n=50.\n",
      "Optimality condition not satisfied for n=60.\n",
      "Optimality condition not satisfied for n=70.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexgiving/Documents/PythonProjects/optimization/venv/lib/python3.11/site-packages/cvxpy/problems/problem.py:1403: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimality condition not satisfied for n=80.\n",
      "Optimality condition not satisfied for n=90.\n"
     ]
    }
   ],
   "source": [
    "dimensionality_range = range(10, 100, 10) # The n = 100 is not supported by solver\n",
    "num_experiments = 1 # N =100\n",
    "\n",
    "\n",
    "for n in dimensionality_range:\n",
    "    mu = np.ones(n)\n",
    "\n",
    "    for _ in range(num_experiments):\n",
    "        matrix_A = get_symmetric_matrix(n)\n",
    "\n",
    "        x = cp.Variable(n)\n",
    "        optimization_func = cp.quad_form(x=(x - mu), P=matrix_A)\n",
    "        objective = cp.Minimize(optimization_func)\n",
    "\n",
    "        constraints = [\n",
    "            cp.sum_squares(x) <= 1\n",
    "        ]\n",
    "\n",
    "        problem = cp.Problem(objective, constraints)\n",
    "        problem.solve(solver=cp.ECOS, abstol=1e-6)\n",
    "\n",
    "        x_opt = x.value\n",
    "\n",
    "        is_optimal = check_optimality(x_opt, matrix_A, mu)\n",
    "        if not is_optimal:\n",
    "            print(f\"Optimality condition not satisfied for n={n}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Для каждого значения $n \\in \\{10, 20,\\dots, 100\\}$ и для каждого тестового примера сгенерируйте 100 начальных точек. В зависимости от варианта реализуйте следующие методы решения задачи (1) для заданной точности $\\epsilon = 0.011$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Given a starting point $x \\in$ dom $f$.\n",
    "\n",
    "While stopping criterion is satisfied:\n",
    "1. Determine a gradient $\\Delta x$.\n",
    "2. *Line search*. Choose a step size $t > 0$.\n",
    "3. Update. $x = x - t\\Delta x$.\n",
    "\n",
    "The stopping criterion is often of the form $||\\nabla f(x)||_{2} \\leq \\eta$, $\\eta > 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact Line Search\n",
    "\n",
    "One line search method sometimes used in practice is exact line search, in which $t$ is chosen to minimize $f$ along the ray $\\{ x - t\\Delta x | t \\geq 0 \\}$\n",
    "\n",
    "$$t = \\argmin_{s \\geq 0}f(x - s \\Delta x)$$\n",
    "\n",
    "An exact line search is used when the cost of the minimization problem with one variable is low compared to the cost of computing the search direction itself. In some special cases the minimizer along the ray can be found analytically, and in others it can be computed efficiently.\n",
    "\n",
    "Usually, it is not possible to do this minimization exactly.\n",
    "Approximations to exact line search are often not much more efficient than backtracking, and it’s not worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_iteration(iteration: int, y: int, x: NDArray) -> None:\n",
    "    print(f'Iteration {iteration}, y: {y}, x: {x}')\n",
    "\n",
    "\n",
    "def get_exact_line_alpha(func: Callable,\n",
    "                         x: NDArray, A: NDArray, mu: NDArray, delta_x: NDArray,\n",
    "                         max_s: float = 1., step_s: float = 0.001) -> float:\n",
    "    s_values = np.arange(0, max_s, step_s)\n",
    "    func_results = [func(x-s*delta_x, A, mu) for s in s_values]\n",
    "    return s_values[np.argmin(func_results)]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GradDescentResult:\n",
    "    x: NDArray\n",
    "    y: float\n",
    "    time: float\n",
    "    iterations: int\n",
    "\n",
    "\n",
    "def grad_desc_exact_line(*,\n",
    "                    n: int,\n",
    "                    A: NDArray,\n",
    "                    func: Callable,\n",
    "                    grad_func: Callable,\n",
    "                    epsilon: float = 1.1e-2,\n",
    "                    max_iters: int = 1000,\n",
    "                    max_s: float = 1.,\n",
    "                    step_s: float = 0.001,\n",
    "                    verbose: bool = False) -> GradDescentResult:\n",
    "    \"\"\"\n",
    "    gradient descent with exact line minimization\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    x = np.random.random(n)\n",
    "    mu = np.ones(n)\n",
    "\n",
    "    if verbose:\n",
    "        print_iteration(0, func(x, A, mu), x)\n",
    "\n",
    "    for iteration in range(1, max_iters+1):\n",
    "\n",
    "        delta_x = grad_func(x, A, mu)\n",
    "        exact_alpha = get_exact_line_alpha(func, x, A, mu, delta_x, max_s, step_s)\n",
    "\n",
    "        x -= exact_alpha * delta_x\n",
    "\n",
    "        if verbose:\n",
    "            print_iteration(iteration, func(x, A, mu), x)\n",
    "\n",
    "        if np.linalg.norm(delta_x) < epsilon:\n",
    "            if verbose:\n",
    "                print('STOP Condition')\n",
    "            break\n",
    "    end_time = time.time()\n",
    "\n",
    "    result = GradDescentResult(x, func(x, A, mu), end_time-start_time, iteration)\n",
    "    if verbose:\n",
    "        print(f'Optimal y: {result.y}, obtained in {result.x}')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[n=10]: Avg time 0.508602 Avg iter 172.33 \n",
      "[n=20]: Avg time 2.211027 Avg iter 728.33 \n"
     ]
    }
   ],
   "source": [
    "n_values = range(10, 21, 10)\n",
    "N = 3\n",
    "\n",
    "for n in n_values:\n",
    "    time_stamps = []\n",
    "    iteration_stamps = []\n",
    "\n",
    "    for experiment in range(N):\n",
    "        A = get_symmetric_matrix(n)\n",
    "        result = grad_desc_exact_line(n=n, A=A, func=function, grad_func=grad_function, verbose=False)\n",
    "\n",
    "        time_stamps.append(result.time)\n",
    "        iteration_stamps.append(result.iterations)\n",
    "\n",
    "    print(f'[n={n}]: Avg time {np.mean(time_stamps):.6f} Avg iter {np.mean(iteration_stamps):.2f} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Объясните принцип работы метода, опишите его преимущества и недостатки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. В качестве результата работы метода:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Для каждого значения $n \\in \\{10,20,\\dots,100\\}$ подсчитайте среднее время работы метода и среднее число итераций (усреднение проводится по всем начальным точкам и по всем тестовым примерам)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Для одного тестового примера при $n = 10$ и нескольких различных начальных точек постройте зависимость тонности от числа итераций. Зависит ли скорость сходимости метода от отношения максимального и минимального собственных чисел матрицы $A$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time for dimension n = 10 is 1.617637\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "N = 3\n",
    "\n",
    "mu = np.ones(n)\n",
    "time_stamps = []\n",
    "\n",
    "for experiment in range(N):\n",
    "    A = get_symmetric_matrix(n)\n",
    "\n",
    "    start_time = time.time()\n",
    "    x = grad_desc_exact_line(n=n, A=A, func=function, grad_func=grad_function)\n",
    "    end_time = time.time()\n",
    "    time_stamps.append(end_time - start_time)\n",
    "\n",
    "    # print(f'Optimal y: {function(x, A, mu)}, obtained in {x}')\n",
    "print(f'Average time for dimension n = {n} is {np.mean(time_stamps):.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
